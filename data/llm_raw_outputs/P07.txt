{
    "paper_id": "P07",
    "repo_name": "code-rag-bench/code-rag-bench",
    "env": {
        "type": "conda",
        "setup_commands": [
            "micromamba create -n crag python=3.10 -y",
            "micromamba activate crag",
            "micromamba install -r requirements.txt -y"
        ]
    },
    "steps": [
        {
            "id": "installation",
            "description": "Install the necessary libraries",
            "working_dir": "",
            "commands": [
                "micromamba install -r requirements.txt -y"
            ],
            "expected_artifacts": []
        },
        {
            "id": "retrieval",
            "description": "Run dense embedding models",
            "working_dir": "retrieval/",
            "commands": [
                "python eval_beir_sbert_canonical.py --model YOUR_MODEL_NAME_OR_PATH --dataset TASK_NAME --output_file PATH_TO_YOUR_SCORE_FILE --results_file PATH_TO_YOUR_RETRIEVAL_RESULTS_FILE"
            ],
            "expected_artifacts": ["retrieval_results_file"]
        },
        {
            "id": "generation",
            "description": "Run code generation",
            "working_dir": "generation/",
            "commands": [
                "python main.py --task TASK_NAME --model MODEL_NAME --dataset_path DATASET_PATH --allow_code_execution"
            ],
            "expected_artifacts": ["generation_outputs"]
        },
        {
            "id": "execution_evaluation",
            "description": "Run execution-based evaluation",
            "working_dir": "generation/",
            "commands": [
                "python main.py --task TASK_NAME --model MODEL_NAME --dataset_path 'json' --data_files_test RETRIEVAL_RESULTS_FILE --metric_output_path results/evaluation_results.json --max_length_input 3596 --max_length_generation 4096 --precision auto --save_every_k_tasks 100 --ignore_eos --model_backend vllm --new_tokens_only --topk_docs 5 --allow_code_execution --load_generations_path GENERATION_OUTPUTS"
            ],
            "expected_artifacts": ["execution_evaluation_results"]
        }
    ]
}
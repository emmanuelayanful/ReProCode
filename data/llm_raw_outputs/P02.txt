{
    "paper_id": "P02",
    "repo_name": "openai/human-eval",
    "env": {
        "type": "conda",
        "setup_commands": [
            "micromamba create -n codex python=3.7 -y",
            "micromamba activate codex"
        ]
    },
    "steps": [
        {
            "id": "installation",
            "description": "Install the repository",
            "working_dir": "~",
            "commands": [
                "git clone https://github.com/openai/human-eval",
                "pip install -e human-eval"
            ],
            "expected_artifacts": [
                "human-eval/LICENSE",
                "human-eval/README.md",
                "human-eval/requirements.txt",
                "human-eval/setup.py",
                "human-eval/data/example_problem.jsonl",
                "human-eval/data/example_samples.jsonl",
                "human-eval/data/HumanEval.jsonl.gz",
                "human-eval/human_eval/__init__.py",
                "human-eval/human_eval/execution.py",
                "human-eval/human_eval/evaluation.py",
                "human-eval/human_eval/evaluate_functional_correctness.py",
                "human-eval/human_eval/data.py"
            ]
        },
        {
            "id": "usage",
            "description": "Run the main experiment",
            "working_dir": "human-eval",
            "commands": [
                "evaluate_functional_correctness data/example_samples.jsonl --problem_file=data/example_problem.jsonl"
            ],
            "expected_artifacts": [
                "human-eval/data/example_samples.jsonl_results.jsonl"
            ]
        }
    ]
}
{
    "paper_id": "P02",
    "repo_name": "openai/human-eval",
    "env": {
        "type": "conda",
        "setup_commands": [
            "conda create -n codex python=3.7 -y",
            "conda activate codex"
        ]
    },
    "steps": [
        {
            "id": "installation",
            "description": "Install the repository and its dependencies",
            "working_dir": ".",
            "commands": [
                "pip install -e human-eval"
            ],
            "expected_artifacts": [
                "human_eval/__init__.py",
                "human_eval/execution.py",
                "human_eval/evaluation.py",
                "human_eval/evaluate_functional_correctness.py",
                "human_eval/data.py"
            ]
        },
        {
            "id": "data_preparation",
            "description": "Prepare the data for evaluation",
            "working_dir": ".",
            "commands": [
                "micromamba run -n codex python human_eval/data.py"
            ],
            "expected_artifacts": [
                "data/example_problem.jsonl",
                "data/example_samples.jsonl",
                "data/HumanEval.jsonl.gz"
            ]
        },
        {
            "id": "evaluation",
            "description": "Evaluate the generated samples",
            "working_dir": ".",
            "commands": [
                "micromamba run -n codex python human_eval/evaluate_functional_correctness.py data/example_samples.jsonl --problem_file=data/example_problem.jsonl"
            ],
            "expected_artifacts": [
                "data/example_samples.jsonl_results.jsonl"
            ]
        }
    ]
}
{
  "paper_id": "P02",
  "repo_name": "openai/human-eval",
  "env": {
    "type": "conda",
    "setup_commands": [
      "micromamba create -n codex python=3.7 -y",
      "micromamba install -n codex pip -y",
      "micromamba run -n codex pip install setuptools<60.0.0 wheel<0.37.0 -y"
    ]
  },
  "steps": [
    {
      "id": "installation",
      "description": "Install the repository and its dependencies",
      "working_dir": "~",
      "commands": [
        "git clone https://github.com/openai/human-eval",
        "micromamba run -n codex pip install -e human-eval"
      ],
      "expected_artifacts": [
        "human-eval"
      ]
    },
    {
      "id": "usage",
      "description": "Run the main experiment",
      "working_dir": "human-eval",
      "commands": [
        "micromamba run -n codex python generate_samples.py"
      ],
      "expected_artifacts": [
        "samples.jsonl"
      ]
    }
  ]
}